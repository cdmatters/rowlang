{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBERT + Rowlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Base & Utility Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def graph_def(f):\n",
    "    '''Cache layer (w self.outputs) and autoscope (w self.name)'''\n",
    "    @wraps(f)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.outputs is None:\n",
    "            with tf.variable_scope(self.name):\n",
    "                self.outputs = f(self, *args, **kwargs)\n",
    "        return self.outputs\n",
    "    return wrapper\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.outputs = None\n",
    "        \n",
    "    @abstractmethod \n",
    "    def on(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self, out_dim, name):\n",
    "        super(LinearLayer, self).__init__(name)\n",
    "        self.out_dim = out_dim\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return tf.layers.dense(X, self.out_dim, activation=None, name=self.name)\n",
    "\n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, dropout, name=\"dropout\"):\n",
    "        super(DropoutLayer, self).__init__(name)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return tf.nn.dropout(X, 1 - self.dropout, name='dropped')   \n",
    "\n",
    "class LayerNormLayer(Layer):\n",
    "    def __init__(self, name=\"layernorm\"):\n",
    "        super(LayerNormLayer, self).__init__(name)\n",
    "        self._eps = 1e-6 # for numerical stability\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        \"\"\"\n",
    "        X: [minibatch x seq x dims]\n",
    "        \"\"\"\n",
    "        self.mean, self.std = tf.nn.moments(X, axes=-1, keep_dims=True)\n",
    "        return (X - self.mean)/ (self.std + self._eps)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MultiHead Attention & FeedFwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(Layer):\n",
    "    def __init__(self, dropout, d_model, d_ff, name=\"f_fwd\"):\n",
    "        super(FeedForwardLayer, self).__init__(name)\n",
    "        \n",
    "        self.d_ff = d_ff\n",
    "        self.linear1 = LinearLayer(self.d_ff, \"ff_up\")\n",
    "        self.linear2 = LinearLayer(d_model, \"ff_down\")\n",
    "        self.dropout = DropoutLayer(dropout)\n",
    "        \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        relu_d = tf.nn.relu(self.linear1.on(X), name=\"relu\")\n",
    "        return self.linear2.on(self.dropout.on(relu_d))\n",
    "           \n",
    "class ScaledDotProdAttentionLayer(Layer):\n",
    "    def __init__(self, scale, dropout, name):\n",
    "        super(ScaledDotProdAttentionLayer, self).__init__(name)\n",
    "        self.scale = scale\n",
    "        self.scores = None\n",
    "        self.dot = None\n",
    "        self.dropout = DropoutLayer(dropout)\n",
    "        \n",
    "    @graph_def\n",
    "    def on(self, Q, K, V):\n",
    "        '''\n",
    "        Q: queries [ minibatch x queries x dim_k]\n",
    "        K: keys    [ minibatch x keys x dim_k]\n",
    "        V: values  [ minibatch x keys x dim_v]\n",
    "        '''\n",
    "        self.dot = tf.einsum('mqd,mkd->mqk', Q, K, name='dot')            \n",
    "        self.scores = tf.nn.softmax(self.scale * self.dot, name='scores') \n",
    "        dropped_scores = self.dropout.on(self.scores)\n",
    "        A = tf.einsum('mqk,mkd->mqd', dropped_scores, V, name='a')\n",
    "        return A\n",
    "    \n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, dropout, d_model, h, name=\"multihead\"):\n",
    "        '''Implement the multiheaded self attention\n",
    "        '''\n",
    "        super(MultiHeadAttention, self).__init__(name)\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.d_model = d_model\n",
    "        self.scale = 1 / np.sqrt(self.d_k)\n",
    "        \n",
    "        self.attentions = []\n",
    "        self.heads = []\n",
    "        Head = namedtuple(\"Head\", [\"to_q\", \"to_k\", \"to_v\", \"attn\"])        \n",
    "        for i in range(h):\n",
    "            q = LinearLayer(self.d_k, \"q\")\n",
    "            k = LinearLayer(self.d_k, \"k\")\n",
    "            v = LinearLayer(self.d_k, \"v\")\n",
    "            attn = ScaledDotProdAttentionLayer(self.scale, dropout, \"attn\")\n",
    "            self.heads.append(Head(q, k, v, attn))\n",
    "        self.A = None\n",
    "        self.out_layer = LinearLayer(self.d_model, \"O\")\n",
    "\n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        for i, h in enumerate(self.heads):\n",
    "            with tf.variable_scope(\"h{}\".format(i)):\n",
    "                q = h.to_q.on(X)                \n",
    "                k = h.to_k.on(X)                \n",
    "                v = h.to_v.on(X)\n",
    "                a = h.attn.on(q,k,v)\n",
    "                self.attentions.append(a)\n",
    "\n",
    "        self.A = tf.concat(self.attentions, axis=-1, name=\"A\")\n",
    "        return self.out_layer.on(self.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Composite Encoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSubLayer(Layer):\n",
    "    def __init__(self, sublayer, dropout, d_model,  name, *args, **kwargs):\n",
    "        super(EncoderSubLayer, self).__init__(name)\n",
    "        self.dropout = DropoutLayer(dropout)\n",
    "        self.sublayer = sublayer(dropout, d_model, *args, **kwargs)\n",
    "        self.layer_norm = LayerNormLayer()\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return X + self.dropout.on(self.sublayer.on(self.layer_norm.on(X)))\n",
    "    \n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, dropout, d_model, heads, d_ff, name):\n",
    "        super(EncoderLayer, self).__init__(name)\n",
    "        self.mha_sublayer = EncoderSubLayer(MultiHeadAttention, dropout, d_model, \n",
    "                                            \"self_attn\", heads)\n",
    "        self.ffwd_sublayer = EncoderSubLayer(FeedForwardLayer, dropout, d_model, \n",
    "                                             \"feed_fwd\", d_ff)\n",
    "    @graph_def    \n",
    "    def on(self, X):\n",
    "        return self.ffwd_sublayer.on(self.mha_sublayer.on(X))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(Layer):\n",
    "    def __init__(self, layers, dropout, d_model, heads, d_ff, name=\"BERT\"):\n",
    "        super(BERTModel, self).__init__(name)\n",
    "        self.layers = []\n",
    "        for i in range(layers):\n",
    "            el = EncoderLayer(dropout, d_model, heads, d_ff, \"layer{}\".format(i))\n",
    "            self.layers.append(el)\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.on(X)\n",
    "        return X\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.86631261 0.21646375 0.89855265 0.69724927 0.34142794]\n",
      "  [0.93314301 0.97796753 0.14437069 0.52645811 0.86292791]\n",
      "  [0.32033719 0.92523595 0.88753367 0.17552919 0.89574283]]\n",
      "\n",
      " [[0.07789924 0.56715091 0.49229367 0.47560545 0.73355563]\n",
      "  [0.5279176  0.65726757 0.0765989  0.90232849 0.88114423]\n",
      "  [0.06543972 0.31373356 0.24131088 0.67517648 0.192631  ]]\n",
      "\n",
      " [[0.71670188 0.08114562 0.22180622 0.17892    0.99999689]\n",
      "  [0.30849568 0.77699325 0.88885983 0.0181805  0.69846696]\n",
      "  [0.43597488 0.57099396 0.18301371 0.48416136 0.07300577]]\n",
      "\n",
      " [[0.57180019 0.90015768 0.89639054 0.42244088 0.6499929 ]\n",
      "  [0.06288385 0.8844261  0.09732825 0.49169101 0.21321628]\n",
      "  [0.87738454 0.45994459 0.69213015 0.941501   0.62377234]]]\n",
      "(4, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.9131644 ,  1.1837215 , -3.9401248 , -1.6667563 ,\n",
       "          4.174741  ],\n",
       "        [ 2.306495  ,  0.15267938,  5.5956373 , -0.3653612 ,\n",
       "          0.08902161],\n",
       "        [ 0.94705117,  2.6214416 , -5.46303   , -1.6032618 ,\n",
       "          4.207371  ]],\n",
       "\n",
       "       [[ 2.3319218 ,  3.0767753 ,  3.67152   ,  0.20346087,\n",
       "         -3.693404  ],\n",
       "        [ 3.1243339 ,  0.8846858 ,  7.0291624 ,  1.7395796 ,\n",
       "         -1.5512298 ],\n",
       "        [ 0.1226456 ,  6.291777  ,  1.2744232 ,  0.33477584,\n",
       "         -0.20087352]],\n",
       "\n",
       "       [[-8.546267  ,  3.0947614 , -0.5664164 , 13.124961  ,\n",
       "         -2.4376817 ],\n",
       "        [-8.39151   ,  3.820796  , -5.1076126 , 10.065827  ,\n",
       "          1.4659433 ],\n",
       "        [ 4.2532444 , -3.639643  , 12.330132  ,  0.28948504,\n",
       "          0.72488177]],\n",
       "\n",
       "       [[-3.4543557 ,  5.0694294 ,  0.6532984 ,  5.4662213 ,\n",
       "         -3.150346  ],\n",
       "        [ 0.07833236, -0.89091146, 13.4288    ,  5.837252  ,\n",
       "         -5.912517  ],\n",
       "        [-2.3734071 ,  1.5151955 , -1.9402819 ,  4.287184  ,\n",
       "          4.381305  ]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_POINTS = 4\n",
    "SEQ = 3\n",
    "MODEL_DIM = 5\n",
    "MODEL_FF = 12\n",
    "HEADS = 3\n",
    "LAYERS = 6\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, MODEL_DIM])\n",
    "d = tf.placeholder(tf.float32, shape=tuple())\n",
    "\n",
    "bert = BERTModel(LAYERS, d, MODEL_DIM, HEADS, MODEL_FF)\n",
    "\n",
    "Y = bert.on(X)\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    x_d = np.random.random((DATA_POINTS, SEQ, MODEL_DIM))\n",
    "    print(x_d)\n",
    "    y = (sess.run(Y, feed_dict={X:x_d , d:0.1}))\n",
    "    file_writer = tf.summary.FileWriter('./logdir', sess.graph)\n",
    "    \n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eweezy",
   "language": "python",
   "name": "eweezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

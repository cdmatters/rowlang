{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBERT + Rowlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, head):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from abc import ABC, abstractmethod\n",
    "import functools\n",
    "\n",
    "\n",
    "def graph_def(f):\n",
    "    '''Cache layer (w self.outputs) and autoscope (w self.name)'''\n",
    "    @wraps(f)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.outputs is None:\n",
    "            with tf.variable_scope(self.name):\n",
    "                self.outputs = f(self, *args, **kwargs)\n",
    "        return self.outputs\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.outputs = None\n",
    "        \n",
    "    @abstractmethod \n",
    "    def on(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self, out_dim, name):\n",
    "        super(LinearLayer, self).__init__(name)\n",
    "        self.out_dim = out_dim\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return tf.layers.dense(X, self.out_dim, activation=None, name=self.name)\n",
    "\n",
    "class ScaledDotProdAttentionLayer(Layer):\n",
    "    def __init__(self, scale, name):\n",
    "        super(ScaledDotProdAttentionLayer, self).__init__(name)\n",
    "        self.scale = scale\n",
    "    @graph_def\n",
    "    def on(self, Q, K, V):\n",
    "        '''\n",
    "        Q: queries [ minibatch x queries x dim_k]\n",
    "        K: keys    [ minibatch x keys x dim_k]\n",
    "        V: values  [ minibatch x keys x dim_v]\n",
    "        '''\n",
    "        dot = tf.einsum('mqd,mkd->mqk', Q, K, name='dot')            \n",
    "        scores = tf.nn.softmax(tf.scalar_mul(self.scale, dot), name='scores') \n",
    "        A = tf.einsum('mqk,mkd->mqd', scores, V, name='a')\n",
    "        return A\n",
    "    \n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_model, dropout=0.1, name=\"multihead\"):\n",
    "        '''Implement the multiheaded self attention\n",
    "        '''\n",
    "        super(MultiHeadAttention, self).__init__(name)\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.d_model = d_model\n",
    "        self.attentions = []\n",
    "        self.A = None\n",
    "        self.O = None\n",
    "\n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        Q = K = V = X # todo: separate\n",
    "        for i in range(self.h):\n",
    "            with tf.variable_scope(\"h{}\".format(i)):\n",
    "                q = LinearLayer(self.d_k, \"q\").on(Q)                \n",
    "                k = LinearLayer(self.d_k, \"k\").on(K)                \n",
    "                v = LinearLayer(self.d_k, \"v\").on(V)\n",
    "                scale = 1 / np.sqrt(self.d_k)\n",
    "                a = ScaledDotProdAttentionLayer(scale, \"attn\").on(q,k,v)\n",
    "                self.attentions.append(a)\n",
    "\n",
    "        self.A = tf.concat(self.attentions, axis=-1, name=\"A\")\n",
    "        self.O = LinearLayer(self.d_model, \"O\").on(self.A)\n",
    "        return self.O\n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormLayer(Layer):\n",
    "    def __init__(self, name=\"layernorm\"):\n",
    "        super(LayerNormLayer, self).__init__(name)\n",
    "        self._eps = 1e-6 # for numerical stability\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        \"\"\"\n",
    "        X: [minibatch x seq x dims]\n",
    "        \"\"\"\n",
    "        self.mean, self.std = tf.nn.moments(X, axes=-1, keep_dims=True)\n",
    "        return (X - self.mean)/ (self.std + self._eps)\n",
    "        \n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, dropout, name=\"dropout\"):\n",
    "        super(DropoutLayer, self).__init__(name)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return tf.nn.dropout(X, 1 - self.dropout, name='dropped') \n",
    "\n",
    "class EncoderSubLayer(Layer):\n",
    "    def __init__(self, dropout, sublayer, name, *args, **kwargs):\n",
    "        super(EncoderSubLayer, self).__init__(name)\n",
    "        self.dropout = dropout\n",
    "        self.sublayer = sublayer(*args, **kwargs)\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        X_n = LayerNormLayer().on(X)\n",
    "        return X + DropoutLayer(self.dropout).on(self.sublayer.on(X_n))\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.44902091 0.80281073 0.00862657 0.14515628 0.0123972 ]\n",
      "  [0.13858129 0.80313088 0.58091599 0.73941974 0.57524398]\n",
      "  [0.80027948 0.93625195 0.99246857 0.51971626 0.17310635]]\n",
      "\n",
      " [[0.22600938 0.68505564 0.46919082 0.14917206 0.1759088 ]\n",
      "  [0.11683096 0.21005736 0.79661539 0.36662984 0.45335137]\n",
      "  [0.43006872 0.79080767 0.6609233  0.15022437 0.89146928]]\n",
      "\n",
      " [[0.03756818 0.93855819 0.56245586 0.79120315 0.0049632 ]\n",
      "  [0.3492735  0.02472538 0.40855029 0.21187245 0.15533361]\n",
      "  [0.24403327 0.42323078 0.86115713 0.38359963 0.42857641]]\n",
      "\n",
      " [[0.24732274 0.25097316 0.57164709 0.08956083 0.17019009]\n",
      "  [0.45170401 0.66593378 0.8022394  0.15200534 0.47501003]\n",
      "  [0.16161879 0.50399882 0.2457969  0.62609678 0.91827856]]]\n",
      "[[[-2.251142    0.88513637 -0.14581622  1.7560604  -1.7967244 ]\n",
      "  [-2.8017209   0.73333037 -0.3401395   0.89860594 -2.2055368 ]\n",
      "  [-2.9691918   0.6631591  -0.4129312   0.53556997 -2.3332217 ]]\n",
      "\n",
      " [[ 2.8255858  -0.51193964  0.27725762 -1.0102865   1.9443855 ]\n",
      "  [ 5.2150383  -1.006813    0.8335986  -0.16755694  4.1490774 ]\n",
      "  [ 3.5150607  -1.4119568   0.2696694  -2.5452466   2.895359  ]]\n",
      "\n",
      " [[-4.375288   -1.7461708  -1.8159118  -8.915059   -3.2661552 ]\n",
      "  [ 3.9937224  -2.7216434   0.30113053 -4.1827054   4.1410246 ]\n",
      "  [ 4.692453   -3.0364375   0.25918746 -5.251733    4.5906143 ]]\n",
      "\n",
      " [[ 5.9359555  -2.3379464   0.5601499  -3.6706734   5.0205836 ]\n",
      "  [11.069076   -1.1137283   2.2634425   3.5125284   8.806328  ]\n",
      "  [11.085332   -1.1207666   2.262629    3.4888701   8.816826  ]]]\n",
      "(4, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-1.8021212 ,  1.687947  , -0.13718966,  1.9012166 ,\n",
       "         -1.7843273 ],\n",
       "        [-2.6631396 ,  1.5364612 ,  0.24077648,  1.6380258 ,\n",
       "         -1.6302929 ],\n",
       "        [-2.1689124 ,  1.599411  ,  0.5795374 ,  1.0552862 ,\n",
       "         -2.1601152 ]],\n",
       "\n",
       "       [[ 3.0515952 ,  0.17311603,  0.74644846, -0.8611144 ,\n",
       "          2.1202943 ],\n",
       "        [ 5.331869  , -0.7967557 ,  1.630214  ,  0.1990729 ,\n",
       "          4.602429  ],\n",
       "        [ 3.9451294 , -0.6211491 ,  0.9305927 , -2.3950222 ,\n",
       "          3.7868283 ]],\n",
       "\n",
       "       [[-4.33772   , -0.80761254, -1.2534559 , -8.123856  ,\n",
       "         -3.261192  ],\n",
       "        [ 4.342996  , -2.696918  ,  0.7096808 , -3.970833  ,\n",
       "          4.296358  ],\n",
       "        [ 4.9364862 , -2.6132066 ,  1.1203446 , -4.868133  ,\n",
       "          5.019191  ]],\n",
       "\n",
       "       [[ 6.183278  , -2.0869732 ,  1.1317971 , -3.5811126 ,\n",
       "          5.190774  ],\n",
       "        [11.52078   , -0.4477945 ,  3.065682  ,  3.6645339 ,\n",
       "          9.281338  ],\n",
       "        [11.246951  , -0.6167678 ,  2.508426  ,  4.114967  ,\n",
       "          9.735105  ]]], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DATA_POINTS = 4\n",
    "SEQ = 3\n",
    "MODEL_DIM = 5\n",
    "HEADS = 3\n",
    "DROPOUT = 1e-20\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, MODEL_DIM])\n",
    "# with tf.variable_scope(\"multihead/\"):\n",
    "#     Y = tf.layers.dense(X, 30, use_bias=False)\n",
    "\n",
    "\n",
    "mha = MultiHeadAttention(HEADS, MODEL_DIM )\n",
    "ES = EncoderSubLayer(DROPOUT, MultiHeadAttention, \"self_attn\", \n",
    "                     HEADS, MODEL_DIM)\n",
    "\n",
    "Y = ES.on(X)\n",
    "Z = ES.sublayer.on(None)\n",
    "\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    x_d = np.random.random((DATA_POINTS, SEQ, MODEL_DIM))\n",
    "    print(x_d)\n",
    "    y = (sess.run(Y, feed_dict={X:x_d}))\n",
    "    z = (sess.run(Z, feed_dict={X:x_d}))\n",
    "    print(z)\n",
    "    file_writer = tf.summary.FileWriter('./logdir', sess.graph)\n",
    "    \n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eweezy",
   "language": "python",
   "name": "eweezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBERT + Rowlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, head):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def graph_def(f):\n",
    "    '''Cache layer (w self.outputs) and autoscope (w self.name)'''\n",
    "    @wraps(f)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.outputs is None:\n",
    "            with tf.variable_scope(self.name):\n",
    "                self.outputs = f(self, *args, **kwargs)\n",
    "        return self.outputs\n",
    "    return wrapper\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.outputs = None\n",
    "        \n",
    "    @abstractmethod \n",
    "    def on(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self, out_dim, name):\n",
    "        super(LinearLayer, self).__init__(name)\n",
    "        self.out_dim = out_dim\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return tf.layers.dense(X, self.out_dim, activation=None, name=self.name)\n",
    "\n",
    "class ScaledDotProdAttentionLayer(Layer):\n",
    "    def __init__(self, scale, name):\n",
    "        super(ScaledDotProdAttentionLayer, self).__init__(name)\n",
    "        self.scale = scale\n",
    "    @graph_def\n",
    "    def on(self, Q, K, V):\n",
    "        '''\n",
    "        Q: queries [ minibatch x queries x dim_k]\n",
    "        K: keys    [ minibatch x keys x dim_k]\n",
    "        V: values  [ minibatch x keys x dim_v]\n",
    "        '''\n",
    "        dot = tf.einsum('mqd,mkd->mqk', Q, K, name='dot')            \n",
    "        scores = tf.nn.softmax(tf.scalar_mul(self.scale, dot), name='scores') \n",
    "        A = tf.einsum('mqk,mkd->mqd', scores, V, name='a')\n",
    "        return A\n",
    "    \n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_model, dropout=0.1, name=\"multihead\"):\n",
    "        '''Implement the multiheaded self attention\n",
    "        '''\n",
    "        super(MultiHeadAttention, self).__init__(name)\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.d_model = d_model\n",
    "        self.attentions = []\n",
    "        self.heads = []\n",
    "        Head = namedtuple(\"Head\", [\"to_q\", \"to_k\", \"to_v\"])\n",
    "        \n",
    "        for i in range(h):\n",
    "            q = LinearLayer(self.d_k, \"q\")\n",
    "            k = LinearLayer(self.d_k, \"k\")\n",
    "            v = LinearLayer(self.d_k, \"v\")\n",
    "            self.heads.append(Head(q, k, v))\n",
    "            \n",
    "        \n",
    "        self.A = None\n",
    "        self.O = None\n",
    "\n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        for h in range(self.heads):\n",
    "            with tf.variable_scope(\"h{}\".format(i)):\n",
    "                q = h.to_q.on(X)                \n",
    "                k = h.to_k.on(X)                \n",
    "                v = h.to_v.on(X)\n",
    "                scale = 1 / np.sqrt(self.d_k)\n",
    "                a = ScaledDotProdAttentionLayer(scale, \"attn\").on(q,k,v)\n",
    "                self.attentions.append(a)\n",
    "\n",
    "        self.A = tf.concat(self.attentions, axis=-1, name=\"A\")\n",
    "        self.O = LinearLayer(self.d_model, \"O\").on(self.A)\n",
    "        return self.O\n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormLayer(Layer):\n",
    "    def __init__(self, name=\"layernorm\"):\n",
    "        super(LayerNormLayer, self).__init__(name)\n",
    "        self._eps = 1e-6 # for numerical stability\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        \"\"\"\n",
    "        X: [minibatch x seq x dims]\n",
    "        \"\"\"\n",
    "        self.mean, self.std = tf.nn.moments(X, axes=-1, keep_dims=True)\n",
    "        return (X - self.mean)/ (self.std + self._eps)\n",
    "        \n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, dropout, name=\"dropout\"):\n",
    "        super(DropoutLayer, self).__init__(name)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        return tf.nn.dropout(X, 1 - self.dropout, name='dropped') \n",
    "\n",
    "class EncoderSubLayer(Layer):\n",
    "    def __init__(self, dropout, sublayer, name, *args, **kwargs):\n",
    "        super(EncoderSubLayer, self).__init__(name)\n",
    "        self.dropout = dropout\n",
    "        self.sublayer = sublayer(*args, **kwargs)\n",
    "    \n",
    "    @graph_def\n",
    "    def on(self, X):\n",
    "        X_n = LayerNormLayer().on(X)\n",
    "        return X + DropoutLayer(self.dropout).on(self.sublayer.on(X_n))\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-305ed97aa27e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                      HEADS, MODEL_DIM)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ace179e90582>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-1443d59588b5>\u001b[0m in \u001b[0;36mon\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerNormLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDropoutLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ace179e90582>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ace179e90582>\u001b[0m in \u001b[0;36mon\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DATA_POINTS = 4\n",
    "SEQ = 3\n",
    "MODEL_DIM = 5\n",
    "HEADS = 3\n",
    "DROPOUT = 0.1\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, MODEL_DIM])\n",
    "# with tf.variable_scope(\"multihead/\"):\n",
    "#     Y = tf.layers.dense(X, 30, use_bias=False)\n",
    "\n",
    "\n",
    "mha = MultiHeadAttention(HEADS, MODEL_DIM )\n",
    "ES = EncoderSubLayer(DROPOUT, MultiHeadAttention, \"self_attn\", \n",
    "                     HEADS, MODEL_DIM)\n",
    "\n",
    "Y = ES.on(X)\n",
    "Z = ES.sublayer.on(None)\n",
    "\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    x_d = np.random.random((DATA_POINTS, SEQ, MODEL_DIM))\n",
    "    print(x_d)\n",
    "    y = (sess.run(Y, feed_dict={X:x_d}))\n",
    "    z = (sess.run(Z, feed_dict={X:x_d}))\n",
    "    print(z)\n",
    "    file_writer = tf.summary.FileWriter('./logdir', sess.graph)\n",
    "    \n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eweezy",
   "language": "python",
   "name": "eweezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
